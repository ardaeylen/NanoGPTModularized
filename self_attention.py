import torch
import torch.nn as nn
import torch.nn.functional as F
# We're trying to achieve all the tokens in the sequence talk to each other. In other words, we want the embeddings
# to carry out information from previous time steps to current time step. Simply averaging the past context does not
# help, we want the tokens adaptively (data dependent) choose what are the other tokens in the sequence informative
# for each other for constructing the context. The way self-attention solves this is the following:
# Every single token (node) in the sequence will emit two vectors. They each will emit a "Query" and a "Key". The
# Query vector (roughly speaking) is "what am ı looking for?" and a key vector is "what do ı contain?". The way we
# have data dependent information from these Query and Key vectors is that we dot product each token's query vector in
# the sequence with each other token's key vector. By doing so, we actually encode the context of sequence by encoding
# a single token's meaning in the sequence by taking dot product of the token's query vector and other tokens' key vectors.

class MultiHeadedSelfAttention(nn.Module):
    def __init__(self, context_length, embedding_size, query_dim, value_dim, num_heads, dropout_rate):
        super(MultiHeadedSelfAttention, self).__init__()
        self.context_length = context_length
        self.embedding_size = embedding_size
        self.query_dim = query_dim
        self.value_dim = value_dim
        self.num_heads = num_heads
        self.dropout_rate = dropout_rate
        self.head_dim = self.query_dim // self.num_heads
        self.value_head_dim = self.value_dim // self.num_heads
        self.query_layer = nn.Linear(embedding_size, self.query_dim, bias = False)
        self.key_layer = nn.Linear(embedding_size, self.query_dim, bias=False)
        self.value_layer = nn.Linear(embedding_size, self.value_dim, bias = False)
        self.linear_transformation = nn.Linear(in_features=self.value_dim, out_features = self.embedding_size, bias=False)
        self.dropout = nn.Dropout(self.dropout_rate)
        self.register_buffer("tril", torch.tril(torch.ones((self.context_length, self.context_length))))
    def forward(self, x): # Assume that x is (B, T (Context Length), Embedding Dim)
        B, T, _ = x.shape
        queries = self.query_layer(x)
        queries = queries.view(B, self.num_heads, T, self.head_dim)
        keys = self.key_layer(x)
        keys = keys.view(B, self.num_heads, T, self.head_dim)
        values = self.value_layer(x)
        values = values.view(B, self.num_heads, T, self.value_head_dim)
        attn_wei = (queries @ keys.transpose(2, 3)) # Outputs -> (B, num_heads, T, T) tensor.
        attn_wei = attn_wei.masked_fill(self.tril == 0, float("-inf")) / (self.head_dim ** 0.5)
        attn_wei = F.softmax(attn_wei, dim = -1)
        attn_wei = self.dropout(attn_wei) # Randomly prevent some of the tokens to communicate for regularization.
        out = attn_wei @ values # Outputs -> (B, num_heads, T, head_dim)
        out = out.transpose(1, 2).contiguous().view(B, T, self.value_dim)
        out = self.linear_transformation(out)
        return out

# Attention is a communication mechanism where you have a number of nodes in a directed graph and every node has some
# vector of information and it gets to aggregate information via a weighted sum from all the nodes that point to it.
# And this is done in data dependent manner. So depending on whatever data is actually stored in each node at any point
# in time. The second note is that there is not a single notion of space therefore attention simply acts over set of
# vectors and so by default these vectors (nodes) have no idea where they are positioned in space and that's why we need
# to encode them positionally.
# Self Attention vs Cross Attention
# The reason this attention is "self-attention" is that is because keys, queries and values are produced from the "same"
# source (so these nodes are self attending) which is input itself in principle attention is much more than that.
# So for example in Encoder-Decoder transformers (especially in the original "Attention is all you need" paper) uses
# Decoder to produce keys and values whereas query pairs to produce Encoder block. In order to decode context represented via
# queries (for conditioning on some context) produced by the encoder block with values and keys generated by decoder in
# autoregressive manner.
class SelfAttentionKarpathy(nn.Module):
    def __init__(self, embedding_size, head_size, context_length, dropout_rate = 0.2):
        super(SelfAttentionKarpathy, self).__init__()
        self.head_size = head_size
        self.embedding_size = embedding_size
        self.context_length = context_length
        self.key = nn.Linear(self.embedding_size, self.head_size, bias = False)
        self.query = nn.Linear(self.embedding_size, self.head_size, bias = False)
        self.value = nn.Linear(self.embedding_size, self.head_size)
        self.register_buffer('tril',torch.tril(torch.ones((self.context_length, self.context_length))))
        # tril here is not a parameter of the module, according to pytorch naming conventions this is called
        # a buffer not a parameter.
        self.dropout = nn.Dropout(dropout_rate)
    def forward(self, x): # Assume that x is (B, T (Context Length), Embedding Dim)
        keys = self.key(x) # Outputs (B, T, #heads)
        queries = self.query(x) # Outputs (B, T, #heads)
        values = self.value(x)
        affinities = queries @ keys.transpose(-2, -1) # (B, T, # heads) @ (B, #heads, T) -> (B, T, T)
        affinities = affinities.masked_fill(self.tril == 0, float('-inf')) / (self.head_size**0.5)
        # The reason why they normalize the attention weights by the query/key size is that dot product of unit gaussian
        # inputs (queries and keys) causes large variance output distribution. This is important because these affinities
        # (relationships between deep token embeddings) are fed into a softmax and it is really important especially in
        # the initialization that these affinities are fairly diffuse. Because of softmax if this dot product between
        # queries and keys takes on very positive and very negative numbers inside it, softmax will converge towards
        # "One-Hot vectors". If we do not normalize the product especially in the initialization, we are basically
        # aggregating information from single node. In encoder block, since you want every single token to interact
        # with each other you don't have to mask preliminary tokens to extract global context information but in the
        # decoder block, model should produce token embeddings sequentially with the context information coming from
        # the encoder. Because when decoding language nodes from the future never talk to past (Because that would
        # give away the answer).
        attn_wei = F.softmax(affinities, dim = -1)
        attn_wei = self.dropout(attn_wei) # Randomly prevent some of the tokens to communicate for regularization.
        output = attn_wei @ values # (B, T, T) @ (B, T, #h) -> (B, T, #h)
        return output

if __name__ == "__main__":
    self_attn = MultiHeadedSelfAttention(256, 64, 128, value_dim=128, num_heads=16, dropout_rate=0.2)
    x = torch.randn((4, 256, 64))
    device = torch.device("cuda")
    self_attn.to(device)
    x = x.to(device)
    output = self_attn(x)
    print(output.shape)
